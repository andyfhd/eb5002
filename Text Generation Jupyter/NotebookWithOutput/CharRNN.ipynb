{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open text file and read in data as `text`\n",
    "with open('data/Grimm_text.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    text = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<the frog king, or iron henry>\\n\\nin olden times when wishing still helped one, there lived a king\\nwho'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Showing the first 100 characters\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the text and map each character to an integer and vice versa\n",
    "\n",
    "# We create two dictionaries:\n",
    "# 1. int2char, which maps integers to characters\n",
    "# 2. char2int, which maps characters to integers\n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the text\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([37,  4, 11, 28, 17, 34,  7,  8, 14, 17, 19, 43,  9, 14, 33, 17,  8,\n",
       "        7, 17, 43,  7,  8,  9, 17, 11, 28,  9,  7,  3, 41, 24, 24, 43,  9,\n",
       "       17,  8, 42, 25, 28,  9, 17,  4, 43,  1, 28, 13, 17, 18, 11, 28,  9,\n",
       "       17, 18, 43, 13, 11, 43,  9, 14, 17, 13,  4, 43, 42, 42, 17, 11, 28,\n",
       "       42, 10, 28, 25, 17,  8,  9, 28, 33, 17,  4, 11, 28,  7, 28, 17, 42,\n",
       "       43, 21, 28, 25, 17, 15, 17, 19, 43,  9, 14, 24, 18, 11,  8])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Showing the first 100 encoded characters\n",
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining method to encode one hot labels\n",
    "#as char by char generation, the vocab size is 68, one hot encode is good enough\n",
    "\n",
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining method to make mini-batches for training, batch is generated by \"reshape\", instead of rolling window\n",
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence       \n",
    "    '''\n",
    "    \n",
    "    batch_size_total = batch_size * seq_length\n",
    "    # total number of batches we can make\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    # Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    # iterate through the array, one sequence at a time (step = sequence length)\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # The targets, shifted by one to right side of x\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring the model\n",
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens #vocab lib\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        #define the LSTM\n",
    "        #LSTM hyperparameter:\n",
    "        #input size = one-hot-encoding size = len of vocab lib\n",
    "        #n_hidden = LSTM output vector size\n",
    "        #n_layers = number of vertically stacked lstm layers.\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        #define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        #define the final, fully-connected output layer\n",
    "        #fc layer is required, as n_hidden <> input size, the lstm output size will be (sequence length, batch size, n_hidden) \n",
    "        #fc output shape is (sequence length, batch size, one-hot-encoding size)\n",
    "        #so that each of the output can correspondon to \"probabilty\" of each char in the the vocab lib\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        #get the outputs and the new hidden state from the lstm\n",
    "        #hidden has two tensor, one for cell state and one for hidden state\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        #pass through a dropout layer\n",
    "        out = self.dropout(r_output)\n",
    "        \n",
    "        # Stack up all the lstm output for all batches, horizontally, \n",
    "        #this is to feed into the format requirement of cross entropy loss function\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        #put output through the fully-connected layer\n",
    "        #no last layer activation at training, as we are using cross entropy loss, which is equivalent to have logsoftmax + nllloss\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring the train method\n",
    "def train(net, data, epochs, batch_size, seq_length_set, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    #set the net to training model, this enables drop out\n",
    "    net.train()\n",
    "    \n",
    "    torch.enable_grad()\n",
    "    \n",
    "    #use adam optimizer and cross entropy loss, as the task is one label classification\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    # % of data is held out for validation\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for seq_length in seq_length_set:\n",
    "        for e in range(epochs):\n",
    "            # initialize hidden state\n",
    "            h = init_hidden(net,batch_size)\n",
    "\n",
    "            for x, y in get_batches(data, batch_size, seq_length):\n",
    "                counter += 1\n",
    "\n",
    "                # One-hot encode our data and make them Torch tensors\n",
    "                x = one_hot_encode(x, n_chars)\n",
    "                inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "                if(train_on_gpu):\n",
    "                    inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                # convert h from torch tensor to normal array, so that backprop will not happen on h            \n",
    "                h = tuple([each.data for each in h])\n",
    "\n",
    "                # zero accumulated gradients\n",
    "                net.zero_grad()\n",
    "\n",
    "                # get the output from the model\n",
    "                output, h = net(inputs, h)\n",
    "\n",
    "                # calculate the loss and perform backprop\n",
    "                loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "                loss.backward()\n",
    "\n",
    "                # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "                nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "                opt.step()\n",
    "\n",
    "                # Do one run of validation on validation set for n batches run.\n",
    "                if counter % print_every == 0:\n",
    "                    # Get validation loss\n",
    "                    val_h = init_hidden(net,batch_size)\n",
    "                    val_losses = []\n",
    "\n",
    "                    #set network to eval mode\n",
    "\n",
    "                    net.eval()\n",
    "                    torch.no_grad()\n",
    "                    val_seq_length=1\n",
    "                    for x, y in get_batches(val_data, batch_size, val_seq_length):\n",
    "                        # One-hot encode our data and make them Torch tensors\n",
    "                        x = one_hot_encode(x, n_chars)\n",
    "                        x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "                        #no need to do this, as there is no backprop anyway\n",
    "                        #val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                        inputs, targets = x, y\n",
    "                        if(train_on_gpu):\n",
    "                            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                        output, val_h = net(inputs, val_h)\n",
    "                        val_loss = criterion(output, targets.view(batch_size*val_seq_length).long())\n",
    "\n",
    "                        #no backward, so model parameter not changed while test on validation set\n",
    "\n",
    "                        val_losses.append(val_loss.item())\n",
    "\n",
    "                    net.train() # reset to train mode after iterationg through validation data\n",
    "\n",
    "                    print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                          \"Step: {}...\".format(counter),\n",
    "                          \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                          \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_hidden(net, batch_size):\n",
    "    ''' Initializes hidden state '''\n",
    "    # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "    # initialized to zero, for hidden state and cell state of LSTM\n",
    "    weight = next(net.parameters()).data\n",
    "\n",
    "    if (train_on_gpu):\n",
    "        hidden = (weight.new(net.n_layers, batch_size, net.n_hidden).zero_().cuda(),\n",
    "                  weight.new(net.n_layers, batch_size, net.n_hidden).zero_().cuda())\n",
    "    else:\n",
    "        hidden = (weight.new(net.n_layers, batch_size, net.n_hidden).zero_(),\n",
    "                  weight.new(net.n_layers, batch_size, net.n_hidden).zero_())\n",
    "\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and print the net\n",
    "n_hidden=512\n",
    "n_layers=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = CharRNN(chars, n_hidden, n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(44, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (fc): Linear(in_features=512, out_features=44, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring the hyperparameters\n",
    "batch_size = 128\n",
    "set=np.linspace(10,190,10)\n",
    "np.random.shuffle(set)\n",
    "seq_length_set = list(map(int,  list(set)))\n",
    "n_epochs = 2 # start smaller if you are just testing initial behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2... Step: 50... Loss: 2.9409... Val Loss: 2.9279\n",
      "Epoch: 2/2... Step: 100... Loss: 2.8278... Val Loss: 2.8253\n",
      "Epoch: 2/2... Step: 150... Loss: 2.3743... Val Loss: 2.3455\n",
      "Epoch: 1/2... Step: 200... Loss: 2.1547... Val Loss: 2.1556\n",
      "Epoch: 1/2... Step: 250... Loss: 2.0560... Val Loss: 2.0182\n",
      "Epoch: 1/2... Step: 300... Loss: 1.9446... Val Loss: 1.9089\n",
      "Epoch: 2/2... Step: 350... Loss: 1.7973... Val Loss: 1.8261\n",
      "Epoch: 2/2... Step: 400... Loss: 1.7460... Val Loss: 1.7413\n",
      "Epoch: 1/2... Step: 450... Loss: 1.6987... Val Loss: 1.6781\n",
      "Epoch: 1/2... Step: 500... Loss: 1.6194... Val Loss: 1.6201\n",
      "Epoch: 1/2... Step: 550... Loss: 1.5857... Val Loss: 1.5812\n",
      "Epoch: 2/2... Step: 600... Loss: 1.5448... Val Loss: 1.5358\n",
      "Epoch: 2/2... Step: 650... Loss: 1.5315... Val Loss: 1.5055\n",
      "Epoch: 1/2... Step: 700... Loss: 1.4987... Val Loss: 1.4917\n",
      "Epoch: 1/2... Step: 750... Loss: 1.4631... Val Loss: 1.4657\n",
      "Epoch: 1/2... Step: 800... Loss: 1.4456... Val Loss: 1.4426\n",
      "Epoch: 1/2... Step: 850... Loss: 1.4431... Val Loss: 1.4245\n",
      "Epoch: 2/2... Step: 900... Loss: 1.3951... Val Loss: 1.4100\n",
      "Epoch: 2/2... Step: 950... Loss: 1.3445... Val Loss: 1.3972\n",
      "Epoch: 2/2... Step: 1000... Loss: 1.3214... Val Loss: 1.3803\n",
      "Epoch: 2/2... Step: 1050... Loss: 1.3847... Val Loss: 1.3659\n",
      "Epoch: 1/2... Step: 1100... Loss: 1.3431... Val Loss: 1.3487\n",
      "Epoch: 2/2... Step: 1150... Loss: 1.3146... Val Loss: 1.3325\n",
      "Epoch: 1/2... Step: 1200... Loss: 1.2989... Val Loss: 1.3195\n",
      "Epoch: 1/2... Step: 1250... Loss: 1.3041... Val Loss: 1.3160\n",
      "Epoch: 2/2... Step: 1300... Loss: 1.2795... Val Loss: 1.3059\n",
      "Epoch: 2/2... Step: 1350... Loss: 1.2830... Val Loss: 1.2969\n",
      "Epoch: 1/2... Step: 1400... Loss: 1.2867... Val Loss: 1.3084\n",
      "Epoch: 1/2... Step: 1450... Loss: 1.3033... Val Loss: 1.3109\n",
      "Epoch: 1/2... Step: 1500... Loss: 1.3472... Val Loss: 1.3040\n",
      "Epoch: 1/2... Step: 1550... Loss: 1.2592... Val Loss: 1.3008\n",
      "Epoch: 1/2... Step: 1600... Loss: 1.3248... Val Loss: 1.2932\n",
      "Epoch: 1/2... Step: 1650... Loss: 1.3218... Val Loss: 1.2923\n",
      "Epoch: 1/2... Step: 1700... Loss: 1.2699... Val Loss: 1.2888\n",
      "Epoch: 2/2... Step: 1750... Loss: 1.1974... Val Loss: 1.2856\n",
      "Epoch: 2/2... Step: 1800... Loss: 1.2503... Val Loss: 1.2779\n",
      "Epoch: 2/2... Step: 1850... Loss: 1.2458... Val Loss: 1.2690\n",
      "Epoch: 2/2... Step: 1900... Loss: 1.2117... Val Loss: 1.2703\n",
      "Epoch: 2/2... Step: 1950... Loss: 1.2210... Val Loss: 1.2637\n",
      "Epoch: 2/2... Step: 2000... Loss: 1.2069... Val Loss: 1.2666\n",
      "Epoch: 2/2... Step: 2050... Loss: 1.2239... Val Loss: 1.2612\n",
      "Epoch: 1/2... Step: 2100... Loss: 1.2886... Val Loss: 1.2920\n",
      "Epoch: 1/2... Step: 2150... Loss: 1.1293... Val Loss: 1.2970\n",
      "Epoch: 1/2... Step: 2200... Loss: 1.2646... Val Loss: 1.2899\n",
      "Epoch: 1/2... Step: 2250... Loss: 1.2220... Val Loss: 1.2886\n",
      "Epoch: 1/2... Step: 2300... Loss: 1.2627... Val Loss: 1.2848\n",
      "Epoch: 1/2... Step: 2350... Loss: 1.2462... Val Loss: 1.2840\n",
      "Epoch: 1/2... Step: 2400... Loss: 1.2536... Val Loss: 1.2769\n",
      "Epoch: 1/2... Step: 2450... Loss: 1.2300... Val Loss: 1.2744\n",
      "Epoch: 1/2... Step: 2500... Loss: 1.2274... Val Loss: 1.2756\n",
      "Epoch: 1/2... Step: 2550... Loss: 1.2744... Val Loss: 1.2700\n",
      "Epoch: 1/2... Step: 2600... Loss: 1.1754... Val Loss: 1.2718\n",
      "Epoch: 1/2... Step: 2650... Loss: 1.2266... Val Loss: 1.2682\n",
      "Epoch: 1/2... Step: 2700... Loss: 1.1826... Val Loss: 1.2640\n",
      "Epoch: 1/2... Step: 2750... Loss: 1.1570... Val Loss: 1.2658\n",
      "Epoch: 1/2... Step: 2800... Loss: 1.2075... Val Loss: 1.2643\n",
      "Epoch: 1/2... Step: 2850... Loss: 1.2601... Val Loss: 1.2616\n",
      "Epoch: 1/2... Step: 2900... Loss: 1.1572... Val Loss: 1.2611\n",
      "Epoch: 1/2... Step: 2950... Loss: 1.1605... Val Loss: 1.2599\n",
      "Epoch: 1/2... Step: 3000... Loss: 1.2683... Val Loss: 1.2569\n",
      "Epoch: 1/2... Step: 3050... Loss: 1.1650... Val Loss: 1.2549\n",
      "Epoch: 2/2... Step: 3100... Loss: 1.2062... Val Loss: 1.2613\n",
      "Epoch: 2/2... Step: 3150... Loss: 1.1629... Val Loss: 1.2570\n",
      "Epoch: 2/2... Step: 3200... Loss: 1.1080... Val Loss: 1.2523\n",
      "Epoch: 2/2... Step: 3250... Loss: 1.1531... Val Loss: 1.2514\n",
      "Epoch: 2/2... Step: 3300... Loss: 1.1853... Val Loss: 1.2501\n",
      "Epoch: 2/2... Step: 3350... Loss: 1.1673... Val Loss: 1.2507\n",
      "Epoch: 2/2... Step: 3400... Loss: 1.2185... Val Loss: 1.2522\n",
      "Epoch: 2/2... Step: 3450... Loss: 1.2878... Val Loss: 1.2455\n",
      "Epoch: 2/2... Step: 3500... Loss: 1.1491... Val Loss: 1.2481\n",
      "Epoch: 2/2... Step: 3550... Loss: 1.1471... Val Loss: 1.2452\n",
      "Epoch: 2/2... Step: 3600... Loss: 1.1238... Val Loss: 1.2436\n",
      "Epoch: 2/2... Step: 3650... Loss: 1.2415... Val Loss: 1.2470\n",
      "Epoch: 2/2... Step: 3700... Loss: 1.1930... Val Loss: 1.2470\n",
      "Epoch: 2/2... Step: 3750... Loss: 1.1948... Val Loss: 1.2438\n",
      "Epoch: 2/2... Step: 3800... Loss: 1.1798... Val Loss: 1.2418\n",
      "Epoch: 2/2... Step: 3850... Loss: 1.2868... Val Loss: 1.2425\n",
      "Epoch: 2/2... Step: 3900... Loss: 1.2647... Val Loss: 1.2401\n",
      "Epoch: 2/2... Step: 3950... Loss: 1.1857... Val Loss: 1.2403\n",
      "Epoch: 2/2... Step: 4000... Loss: 1.1450... Val Loss: 1.2382\n",
      "Epoch: 2/2... Step: 4050... Loss: 1.2180... Val Loss: 1.2413\n",
      "Epoch: 2/2... Step: 4100... Loss: 1.1910... Val Loss: 1.2395\n",
      "Epoch: 1/2... Step: 4150... Loss: 1.1342... Val Loss: 1.2226\n",
      "Epoch: 2/2... Step: 4200... Loss: 1.1366... Val Loss: 1.2131\n",
      "Epoch: 2/2... Step: 4250... Loss: 1.1407... Val Loss: 1.2091\n",
      "Epoch: 1/2... Step: 4300... Loss: 1.1197... Val Loss: 1.2067\n",
      "Epoch: 2/2... Step: 4350... Loss: 1.1017... Val Loss: 1.2038\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length_set=seq_length_set, lr=0.001, print_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "model_name = 'model/CharRNN.pt'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a method to generate the next character\n",
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        \n",
    "        #input is one hot encoding of a single character\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # during generation, the hidden state is not reset\n",
    "        # instead, it will always take the hidden state from previous char output and use it as input for this char\n",
    "        # h = tuple([each.data for each in h])\n",
    "        \n",
    "        # get the output of the model\n",
    "        #during training, sequence length is > 1, but during prediction, sequence length is 1.\n",
    "        #so each time it will generate one char, but because the hidden state is not cleared, it can still\n",
    "        #remember information from all previous characters\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities, here we use softmax\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring a method to generate new text\n",
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = init_hidden(net,1)\n",
    "    \n",
    "    #run but not use the predicted output, only keep the hidden states\n",
    "    for ch in prime:\n",
    "        char, h, = predict(net, ch, h, top_k=top_k)\n",
    "    \n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h, = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hero in the wood>\n",
      "\n",
      "there was once once a little hand, and said, there is no money\n",
      "time\n",
      "an instant.  i have not to say\n",
      "to st. peter, when she saw that the musician heard the bed and wanted\n",
      "to say, but she was now so much as sould not be the soldiers, who was not\n",
      "still like it brother.  the wedding had been still looking on the rest, and was forced to speak for a little boy on her father.\n",
      "the shoped her sharp stayed into the way he had to come at onger, and he took her away again, and then an her servant was set a false, and then she that they had to could have had been believed what the window saw the hole again in an instant in the water, and when she had she said, this mean was the three daughter with all the castle\n",
      "with half one\n",
      "will be so sud or if they will be free, but\n",
      "i will go on,\"\n",
      "said he.\n",
      "     what deep me\n",
      "the strung.  and this wanted to gave him her hands, and the two stones was\n",
      "this sore to be comfuring them, and\n",
      "said, you must but\n",
      "see him and to will you.  i will stay here and threw you and say, she is a little fish, and the king's daughter had to see\n",
      "the wild.  thou a shepter to stay the beautiful brother and there went and bade him, and she was a son again.\n",
      "that in the feathers humal and boy had good beautiful and how she set out, and the king\n",
      "was sitting on this sack and three tanissed with all miserable\n",
      "stream, but and the woman went and seek off the tall other as it said, if i want.  then the king had taken her a smoth one of his\n",
      "bed went into the carriage of the world.  and well\n",
      "that they came through to his work, but when the son had given his bed and\n",
      "said, teer yourself in the world without satisfied, so that they will set their life.  in the chormy a white world should not give her she will be fallen in the forest, but so\n",
      "they\n",
      "had a\n",
      "lange to be thought that the\n",
      "stopp, however, had a little hill and she heard that, he said, where i am i am tired in a chorst.  and he was about to gave him a servant through all the brooms, and went his bed, and had a broom who\n",
      "had\n",
      "t\n"
     ]
    }
   ],
   "source": [
    "# Generating new text\n",
    "print(sample(net, 2000, prime='<hero in the wood>', top_k=5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
