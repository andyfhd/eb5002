{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from io import open\n",
    "import torch\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        # training config\n",
    "        self.data = 'data/Grimm_text.txt' # path to data text\n",
    "        self.val_frac = 0.1 # fraction of validation\n",
    "        self.model = 'GRU' # type of recurrent net (RNN_TANH, RNN_RELU, LSTM, GRU)\n",
    "        self.emsize = 200 # size of word embeddings\n",
    "        self.nhid = 200 # number of hidden units per layer\n",
    "        self.nlayers = 2 # number of layers\n",
    "        self.lr = 20 # initial learning rate\n",
    "        self.clip = 0.25 # gradient clipping\n",
    "        self.epochs = 40 # upper epochs limit\n",
    "        self.batch_size = 20 # batch size\n",
    "        self.bptt = 35 # sequence length\n",
    "        self.dropout = 0.2 # dropout applied to layers (0 = no dropout)\n",
    "        self.seed = 1111 # random seed\n",
    "        self.log_interval = 200 # report interval\n",
    "        self.save = 'model/WordRNN.pt' # path to save the final model\n",
    "        \n",
    "        # generation config\n",
    "        self.words = 1000 # number of words to generate\n",
    "        self.temperature = 1.0 # temperature for generation - higher will increase diversity\n",
    "        self.outf = 'output/generated.txt' # output file for generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x174dbea3db0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the random seed manually for reproducibility.\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if train_on_gpu else \"cpu\")\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maintain mapping between words and indices\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader that reads input text, splits into train and validation and maintain word indices\n",
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        # Open text file and read in data as `text`\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        val_idx = int(len(lines)*(1-args.val_frac))\n",
    "        train_lines, val_lines = lines[:val_idx], lines[val_idx:]\n",
    "        self.train = self.tokenize(train_lines)\n",
    "        self.valid = self.tokenize(val_lines)\n",
    "\n",
    "    def tokenize(self, lines):\n",
    "        tokens = 0\n",
    "        for line in lines:\n",
    "            words = line.split() + ['<eos>']\n",
    "            tokens += len(words)\n",
    "            for word in words:\n",
    "                self.dictionary.add_word(word)\n",
    "\n",
    "        ids = torch.LongTensor(tokens)\n",
    "        token = 0\n",
    "        for line in lines:\n",
    "            words = line.split() + ['<eos>']\n",
    "            for word in words:\n",
    "                ids[token] = self.dictionary.word2idx[word]\n",
    "                token += 1\n",
    "\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "corpus = Corpus(args.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting from sequential data, batchify arranges the dataset into columns.\n",
    "# For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
    "# ┌ a g m s ┐\n",
    "# │ b h n t │\n",
    "# │ c i o u │\n",
    "# │ d j p v │\n",
    "# │ e k q w │\n",
    "# └ f l r x ┘.\n",
    "# These columns are treated as independent by the model, which means that the\n",
    "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\n",
    "# batch processing.\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_batch_size = 10\n",
    "train_data = batchify(corpus.train, args.batch_size)\n",
    "val_data = batchify(corpus.valid, eval_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordRNN(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
    "        super(WordRNN, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        if rnn_type in ['LSTM', 'GRU']:\n",
    "            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n",
    "        else:\n",
    "            try:\n",
    "                nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n",
    "            except KeyError:\n",
    "                raise ValueError( \"\"\"An invalid option for `--model` was supplied,\n",
    "                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\")\n",
    "            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
    "                    weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
    "        else:\n",
    "            return weight.new_zeros(self.nlayers, bsz, self.nhid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(corpus.dictionary)\n",
    "model = WordRNN(args.model, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_batch subdivides the source data into chunks of length args.bptt.\n",
    "# If source is equal to the example output of the batchify function, with\n",
    "# a bptt-limit of 2, we'd get the following two Variables for i = 0:\n",
    "# ┌ a g m s ┐ ┌ b h n t ┐\n",
    "# └ b h n t ┘ └ c i o u ┘\n",
    "# Note that despite the name of the function, the subdivison of data is not\n",
    "# done along the batch dimension (i.e. dimension 1), since that was handled\n",
    "# by the batchify function. The chunks are along dimension 0, corresponding\n",
    "# to the seq_len dimension in the LSTM.\n",
    "\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(args.bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, args.bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output, hidden = model(data, hidden)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "            hidden = repackage_hidden(hidden)\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(args.batch_size)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, args.bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % args.log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / args.log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // args.bptt, lr,\n",
    "                elapsed * 1000 / args.log_interval, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over epochs.\n",
    "lr = args.lr\n",
    "best_val_loss = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/  393 batches | lr 20.00 | ms/batch 31.28 | loss  8.66 | ppl  5744.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 14.82s | valid loss  6.41 | valid ppl   607.68\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dongdong\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:251: UserWarning: Couldn't retrieve source code for container of type WordRNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |   200/  393 batches | lr 20.00 | ms/batch 29.18 | loss  5.89 | ppl   360.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 11.87s | valid loss  5.65 | valid ppl   284.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/  393 batches | lr 20.00 | ms/batch 28.67 | loss  5.20 | ppl   181.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 11.71s | valid loss  5.35 | valid ppl   211.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/  393 batches | lr 20.00 | ms/batch 29.09 | loss  4.90 | ppl   134.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 11.88s | valid loss  5.27 | valid ppl   194.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/  393 batches | lr 20.00 | ms/batch 34.15 | loss  4.71 | ppl   110.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 14.13s | valid loss  5.22 | valid ppl   184.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   200/  393 batches | lr 20.00 | ms/batch 33.72 | loss  4.57 | ppl    96.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 13.51s | valid loss  5.24 | valid ppl   188.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   200/  393 batches | lr 5.00 | ms/batch 32.21 | loss  4.27 | ppl    71.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 12.95s | valid loss  5.06 | valid ppl   158.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   200/  393 batches | lr 5.00 | ms/batch 31.45 | loss  4.11 | ppl    60.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 12.81s | valid loss  5.06 | valid ppl   157.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |   200/  393 batches | lr 5.00 | ms/batch 31.82 | loss  4.03 | ppl    56.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 12.94s | valid loss  5.07 | valid ppl   158.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   200/  393 batches | lr 1.25 | ms/batch 32.81 | loss  3.94 | ppl    51.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 13.19s | valid loss  5.05 | valid ppl   155.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |   200/  393 batches | lr 1.25 | ms/batch 31.99 | loss  3.90 | ppl    49.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 13.01s | valid loss  5.05 | valid ppl   155.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |   200/  393 batches | lr 1.25 | ms/batch 32.06 | loss  3.86 | ppl    47.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 13.03s | valid loss  5.05 | valid ppl   155.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |   200/  393 batches | lr 0.31 | ms/batch 32.24 | loss  3.84 | ppl    46.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 13.85s | valid loss  5.04 | valid ppl   154.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |   200/  393 batches | lr 0.31 | ms/batch 36.54 | loss  3.83 | ppl    46.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 14.86s | valid loss  5.04 | valid ppl   154.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |   200/  393 batches | lr 0.31 | ms/batch 36.54 | loss  3.82 | ppl    45.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 14.91s | valid loss  5.04 | valid ppl   154.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |   200/  393 batches | lr 0.08 | ms/batch 39.07 | loss  3.81 | ppl    45.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 15.97s | valid loss  5.04 | valid ppl   154.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |   200/  393 batches | lr 0.08 | ms/batch 40.31 | loss  3.81 | ppl    45.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 15.72s | valid loss  5.04 | valid ppl   154.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |   200/  393 batches | lr 0.08 | ms/batch 36.65 | loss  3.81 | ppl    45.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 14.90s | valid loss  5.04 | valid ppl   154.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |   200/  393 batches | lr 0.08 | ms/batch 37.09 | loss  3.81 | ppl    45.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 14.96s | valid loss  5.04 | valid ppl   154.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |   200/  393 batches | lr 0.02 | ms/batch 36.54 | loss  3.81 | ppl    44.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 14.94s | valid loss  5.04 | valid ppl   154.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  21 |   200/  393 batches | lr 0.00 | ms/batch 36.82 | loss  3.81 | ppl    44.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 14.95s | valid loss  5.04 | valid ppl   154.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  22 |   200/  393 batches | lr 0.00 | ms/batch 36.59 | loss  3.80 | ppl    44.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 14.90s | valid loss  5.04 | valid ppl   154.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  23 |   200/  393 batches | lr 0.00 | ms/batch 36.60 | loss  3.80 | ppl    44.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 14.88s | valid loss  5.04 | valid ppl   154.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  24 |   200/  393 batches | lr 0.00 | ms/batch 36.66 | loss  3.81 | ppl    44.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 14.92s | valid loss  5.04 | valid ppl   154.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  25 |   200/  393 batches | lr 0.00 | ms/batch 36.55 | loss  3.81 | ppl    44.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 14.86s | valid loss  5.04 | valid ppl   154.41\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  26 |   200/  393 batches | lr 0.00 | ms/batch 36.62 | loss  3.80 | ppl    44.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 15.00s | valid loss  5.04 | valid ppl   154.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  27 |   200/  393 batches | lr 0.00 | ms/batch 36.65 | loss  3.80 | ppl    44.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 15.15s | valid loss  5.04 | valid ppl   154.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  28 |   200/  393 batches | lr 0.00 | ms/batch 36.79 | loss  3.81 | ppl    45.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 15.08s | valid loss  5.04 | valid ppl   154.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  29 |   200/  393 batches | lr 0.00 | ms/batch 37.18 | loss  3.81 | ppl    45.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 15.11s | valid loss  5.04 | valid ppl   154.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  30 |   200/  393 batches | lr 0.00 | ms/batch 37.74 | loss  3.80 | ppl    44.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 15.38s | valid loss  5.04 | valid ppl   154.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  31 |   200/  393 batches | lr 0.00 | ms/batch 37.82 | loss  3.80 | ppl    44.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 15.38s | valid loss  5.04 | valid ppl   154.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  32 |   200/  393 batches | lr 0.00 | ms/batch 37.89 | loss  3.81 | ppl    44.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 15.21s | valid loss  5.04 | valid ppl   154.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  33 |   200/  393 batches | lr 0.00 | ms/batch 36.50 | loss  3.81 | ppl    45.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 14.87s | valid loss  5.04 | valid ppl   154.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  34 |   200/  393 batches | lr 0.00 | ms/batch 36.55 | loss  3.80 | ppl    44.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 14.89s | valid loss  5.04 | valid ppl   154.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  35 |   200/  393 batches | lr 0.00 | ms/batch 36.50 | loss  3.81 | ppl    45.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 14.85s | valid loss  5.04 | valid ppl   154.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  36 |   200/  393 batches | lr 0.00 | ms/batch 36.51 | loss  3.81 | ppl    45.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time: 14.86s | valid loss  5.04 | valid ppl   154.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  37 |   200/  393 batches | lr 0.00 | ms/batch 36.52 | loss  3.81 | ppl    44.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time: 14.87s | valid loss  5.04 | valid ppl   154.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  38 |   200/  393 batches | lr 0.00 | ms/batch 36.51 | loss  3.81 | ppl    45.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time: 14.85s | valid loss  5.04 | valid ppl   154.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  39 |   200/  393 batches | lr 0.00 | ms/batch 36.57 | loss  3.81 | ppl    45.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time: 14.91s | valid loss  5.04 | valid ppl   154.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  40 |   200/  393 batches | lr 0.00 | ms/batch 36.41 | loss  3.80 | ppl    44.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time: 14.98s | valid loss  5.04 | valid ppl   154.41\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# At any point you can use Kernel->Interrupt break out of training early.\n",
    "try:\n",
    "    for epoch in range(1, args.epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        train()\n",
    "        val_loss = evaluate(val_data)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                           val_loss, math.exp(val_loss)))\n",
    "        print('-' * 89)\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            with open(args.save, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "            lr /= 4.0\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best saved model.\n",
    "with open(args.save, 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "    # after load the rnn params are not a continuous chunk of memory\n",
    "    # this makes them a continuous chunk, and will speed up forward pass\n",
    "    model.rnn.flatten_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "hidden = model.init_hidden(1)\n",
    "input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Generated 0/1000 words\n",
      "| Generated 200/1000 words\n",
      "| Generated 400/1000 words\n",
      "| Generated 600/1000 words\n",
      "| Generated 800/1000 words\n"
     ]
    }
   ],
   "source": [
    "with open(args.outf, 'w') as outf:    \n",
    "    with torch.no_grad():  # no tracking history\n",
    "        for i in range(args.words):\n",
    "            output, hidden = model(input, hidden)\n",
    "            word_weights = output.squeeze().div(args.temperature).exp().cpu()\n",
    "            word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "            input.fill_(word_idx)\n",
    "            word = corpus.dictionary.idx2word[word_idx]\n",
    "\n",
    "            outf.write(word + ('\\n' if i % 20 == 19 else ' '))\n",
    "\n",
    "            if i % args.log_interval == 0:\n",
    "                print('| Generated {}/{} words'.format(i, args.words))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
